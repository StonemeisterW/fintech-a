{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RNNs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Unrolling Through Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1],\n",
    "                                                dtype=tf.float32)\n",
    "Y0, Y1 = output_seqs\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Unrolling Through Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "X_batch = np.array([\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 2\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Variable Length Input Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "\n",
    "\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,\n",
    "                                    sequence_length=seq_length)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "seq_length_batch = np.array([2, 1, 2, 2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run(\n",
    "        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trining RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Sequence Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trining to Predict Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_outputs = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "    tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),\n",
    "    output_size=n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(outputs - y)) # MSE\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iterations = 1500\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if iteration % 100 == 0:\n",
    "            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)\n",
    "    \n",
    "    saver.save(sess, \"./my_time_series_model\") # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                          # not shown in the book\n",
    "    saver.restore(sess, \"./my_time_series_model\")   # not shown\n",
    "\n",
    "    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))\n",
    "    y_pred = sess.run(outputs, feed_dict={X: X_new})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_neurons = 100\n",
    "n_layers = 3\n",
    "\n",
    "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "          for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder_with_default(1.0, shape=())\n",
    "cells = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "         for layer in range(n_layers)]\n",
    "cells_drop = [tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)\n",
    "              for cell in cells]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells_drop)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing tha data - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/movie_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-51537ae3fa94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/movie_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tako/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tako/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tako/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tako/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tako/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/movie_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/movie_data.csv', encoding='utf-8')\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing tha data - 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:23\n",
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------------------------------------------'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprocessing the data:\n",
    "## Separate words and \n",
    "## count each word's occurrence\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Counting words occurences')\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' ' \\\n",
    "                    for c in review]).lower()\n",
    "    df.loc[i,'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "    \n",
    "## Create a mapping:\n",
    "## Map each unique word to an integer\n",
    "\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing tha data - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--------------------------------------------'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define fixed-length sequences:\n",
    "## Use the last 200 elements of each sequence\n",
    "## if sequence length < 200: left-pad with zeros\n",
    "\n",
    "sequence_length = 200  ## sequence length (or T in our formulas)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing tha data - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--------------------------------------------'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "\n",
    "np.random.seed(123) # for reproducibility\n",
    "\n",
    "## Function to generate minibatches:\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x= x[:n_batches*batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--------------------------------------------'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                 lstm_size=256, num_layers=1, batch_size=64,\n",
    "                 learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size   ## number of hidden units\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def build(self):\n",
    "        ## Define the placeholders\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                    shape=(self.batch_size, self.seq_len),\n",
    "                    name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32,\n",
    "                    shape=(self.batch_size),\n",
    "                    name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                    name='tf_keepprob')\n",
    "        ## Create the embedding layer\n",
    "        embedding = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        (self.n_words, self.embed_size),\n",
    "                        minval=-1, maxval=1),\n",
    "                    name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(\n",
    "                    embedding, tf_x, \n",
    "                    name='embeded_x')\n",
    "\n",
    "        ## Define LSTM cell and stack them together\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "                [tf.contrib.rnn.DropoutWrapper(\n",
    "                   tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                   output_keep_prob=tf_keepprob)\n",
    "                 for i in range(self.num_layers)])\n",
    "\n",
    "        ## Define the initial state:\n",
    "        self.initial_state = cells.zero_state(\n",
    "                 self.batch_size, tf.float32)\n",
    "        print('  << initial state >> ', self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                 cells, embed_x,\n",
    "                 initial_state=self.initial_state)\n",
    "        ## Note: lstm_outputs shape: \n",
    "        ##  [batch_size, max_time, cells.output_size]\n",
    "        print('\\n  << lstm_output   >> ', lstm_outputs)\n",
    "        print('\\n  << final state   >> ', self.final_state)\n",
    "\n",
    "        ## Apply a FC layer after on top of RNN output:\n",
    "        logits = tf.layers.dense(\n",
    "                 inputs=lstm_outputs[:, -1],\n",
    "                 units=1, activation=None,\n",
    "                 name='logits')\n",
    "        \n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print ('\\n  << logits        >> ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels' : tf.cast(tf.round(y_proba), tf.int32,\n",
    "                 name='labels')\n",
    "        }\n",
    "        print('\\n  << predictions   >> ', predictions)\n",
    "\n",
    "        ## Define the cost function\n",
    "        cost = tf.reduce_mean(\n",
    "                 tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                 labels=tf_y, logits=logits),\n",
    "                 name='cost')\n",
    "        \n",
    "        ## Define the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                            X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state : state}\n",
    "                    loss, _, state = sess.run(\n",
    "                            ['cost:0', 'train_op', \n",
    "                             self.final_state],\n",
    "                            feed_dict=feed)\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                              \"| Train loss: %.5f\" % (\n",
    "                               epoch + 1, num_epochs,\n",
    "                               iteration, loss))\n",
    "\n",
    "                    iteration +=1\n",
    "                if (epoch+1)%10 == 0:\n",
    "                    self.saver.save(sess,\n",
    "                        \"model/sentiment-%d.ckpt\" % epoch)\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, tf.train.latest_checkpoint('model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(\n",
    "                create_batch_generator(\n",
    "                    X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0' : batch_x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state : test_state}\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                    \n",
    "                preds.append(pred)\n",
    "                \n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << lstm_output   >>  Tensor(\"rnn/transpose:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      "  << final state   >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << logits        >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << predictions   >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n",
      "Epoch: 1/40 Iteration: 20 | Train loss: 0.68530\n",
      "Epoch: 1/40 Iteration: 40 | Train loss: 0.67821\n",
      "Epoch: 1/40 Iteration: 60 | Train loss: 0.56897\n",
      "Epoch: 1/40 Iteration: 80 | Train loss: 0.52177\n",
      "Epoch: 1/40 Iteration: 100 | Train loss: 0.54995\n",
      "Epoch: 1/40 Iteration: 120 | Train loss: 0.50090\n",
      "Epoch: 1/40 Iteration: 140 | Train loss: 0.55078\n",
      "Epoch: 1/40 Iteration: 160 | Train loss: 0.53099\n",
      "Epoch: 1/40 Iteration: 180 | Train loss: 0.40085\n",
      "Epoch: 1/40 Iteration: 200 | Train loss: 0.52015\n",
      "Epoch: 1/40 Iteration: 220 | Train loss: 0.49581\n",
      "Epoch: 1/40 Iteration: 240 | Train loss: 0.40987\n",
      "Epoch: 2/40 Iteration: 260 | Train loss: 0.36045\n",
      "Epoch: 2/40 Iteration: 280 | Train loss: 0.35210\n",
      "Epoch: 2/40 Iteration: 300 | Train loss: 0.35310\n",
      "Epoch: 2/40 Iteration: 320 | Train loss: 0.42725\n",
      "Epoch: 2/40 Iteration: 340 | Train loss: 0.35647\n",
      "Epoch: 2/40 Iteration: 360 | Train loss: 0.30652\n",
      "Epoch: 2/40 Iteration: 380 | Train loss: 0.31385\n",
      "Epoch: 2/40 Iteration: 400 | Train loss: 0.36742\n",
      "Epoch: 2/40 Iteration: 420 | Train loss: 0.35023\n",
      "Epoch: 2/40 Iteration: 440 | Train loss: 0.30801\n",
      "Epoch: 2/40 Iteration: 460 | Train loss: 0.33169\n",
      "Epoch: 2/40 Iteration: 480 | Train loss: 0.36571\n",
      "Epoch: 2/40 Iteration: 500 | Train loss: 0.35710\n",
      "Epoch: 3/40 Iteration: 520 | Train loss: 0.28805\n",
      "Epoch: 3/40 Iteration: 540 | Train loss: 0.20823\n",
      "Epoch: 3/40 Iteration: 560 | Train loss: 0.34625\n",
      "Epoch: 3/40 Iteration: 580 | Train loss: 0.35996\n",
      "Epoch: 3/40 Iteration: 600 | Train loss: 0.22821\n",
      "Epoch: 3/40 Iteration: 620 | Train loss: 0.29517\n",
      "Epoch: 3/40 Iteration: 640 | Train loss: 0.35150\n",
      "Epoch: 3/40 Iteration: 660 | Train loss: 0.25993\n",
      "Epoch: 3/40 Iteration: 680 | Train loss: 0.20735\n",
      "Epoch: 3/40 Iteration: 700 | Train loss: 0.23371\n",
      "Epoch: 3/40 Iteration: 720 | Train loss: 0.30212\n",
      "Epoch: 3/40 Iteration: 740 | Train loss: 0.22922\n",
      "Epoch: 4/40 Iteration: 760 | Train loss: 0.17527\n",
      "Epoch: 4/40 Iteration: 780 | Train loss: 0.08098\n",
      "Epoch: 4/40 Iteration: 800 | Train loss: 0.14379\n",
      "Epoch: 4/40 Iteration: 820 | Train loss: 0.49389\n",
      "Epoch: 4/40 Iteration: 840 | Train loss: 0.17365\n",
      "Epoch: 4/40 Iteration: 860 | Train loss: 0.13266\n",
      "Epoch: 4/40 Iteration: 880 | Train loss: 0.20739\n",
      "Epoch: 4/40 Iteration: 900 | Train loss: 0.19036\n",
      "Epoch: 4/40 Iteration: 920 | Train loss: 0.17630\n",
      "Epoch: 4/40 Iteration: 940 | Train loss: 0.20140\n",
      "Epoch: 4/40 Iteration: 960 | Train loss: 0.10113\n",
      "Epoch: 4/40 Iteration: 980 | Train loss: 0.14634\n",
      "Epoch: 4/40 Iteration: 1000 | Train loss: 0.23192\n",
      "Epoch: 5/40 Iteration: 1020 | Train loss: 0.19845\n",
      "Epoch: 5/40 Iteration: 1040 | Train loss: 0.10983\n",
      "Epoch: 5/40 Iteration: 1060 | Train loss: 0.14113\n",
      "Epoch: 5/40 Iteration: 1080 | Train loss: 0.17896\n",
      "Epoch: 5/40 Iteration: 1100 | Train loss: 0.15376\n",
      "Epoch: 5/40 Iteration: 1120 | Train loss: 0.08996\n",
      "Epoch: 5/40 Iteration: 1140 | Train loss: 0.19269\n",
      "Epoch: 5/40 Iteration: 1160 | Train loss: 0.06062\n",
      "Epoch: 5/40 Iteration: 1180 | Train loss: 0.07847\n",
      "Epoch: 5/40 Iteration: 1200 | Train loss: 0.17042\n",
      "Epoch: 5/40 Iteration: 1220 | Train loss: 0.12355\n",
      "Epoch: 5/40 Iteration: 1240 | Train loss: 0.11576\n",
      "Epoch: 6/40 Iteration: 1260 | Train loss: 0.13160\n",
      "Epoch: 6/40 Iteration: 1280 | Train loss: 0.15040\n",
      "Epoch: 6/40 Iteration: 1300 | Train loss: 0.12333\n",
      "Epoch: 6/40 Iteration: 1320 | Train loss: 0.24192\n",
      "Epoch: 6/40 Iteration: 1340 | Train loss: 0.12353\n",
      "Epoch: 6/40 Iteration: 1360 | Train loss: 0.03367\n",
      "Epoch: 6/40 Iteration: 1380 | Train loss: 0.13684\n",
      "Epoch: 6/40 Iteration: 1400 | Train loss: 0.12766\n",
      "Epoch: 6/40 Iteration: 1420 | Train loss: 0.08701\n",
      "Epoch: 6/40 Iteration: 1440 | Train loss: 0.06397\n",
      "Epoch: 6/40 Iteration: 1460 | Train loss: 0.11066\n",
      "Epoch: 6/40 Iteration: 1480 | Train loss: 0.06899\n",
      "Epoch: 6/40 Iteration: 1500 | Train loss: 0.11311\n",
      "Epoch: 7/40 Iteration: 1520 | Train loss: 0.11989\n",
      "Epoch: 7/40 Iteration: 1540 | Train loss: 0.12266\n",
      "Epoch: 7/40 Iteration: 1560 | Train loss: 0.09051\n",
      "Epoch: 7/40 Iteration: 1580 | Train loss: 0.01663\n",
      "Epoch: 7/40 Iteration: 1600 | Train loss: 0.14574\n",
      "Epoch: 7/40 Iteration: 1620 | Train loss: 0.05041\n",
      "Epoch: 7/40 Iteration: 1640 | Train loss: 0.08205\n",
      "Epoch: 7/40 Iteration: 1660 | Train loss: 0.21443\n",
      "Epoch: 7/40 Iteration: 1680 | Train loss: 0.13687\n",
      "Epoch: 7/40 Iteration: 1700 | Train loss: 0.07148\n",
      "Epoch: 7/40 Iteration: 1720 | Train loss: 0.02403\n",
      "Epoch: 7/40 Iteration: 1740 | Train loss: 0.06062\n",
      "Epoch: 8/40 Iteration: 1760 | Train loss: 0.09601\n",
      "Epoch: 8/40 Iteration: 1780 | Train loss: 0.02654\n",
      "Epoch: 8/40 Iteration: 1800 | Train loss: 0.05824\n",
      "Epoch: 8/40 Iteration: 1820 | Train loss: 0.13487\n",
      "Epoch: 8/40 Iteration: 1840 | Train loss: 0.06702\n",
      "Epoch: 8/40 Iteration: 1860 | Train loss: 0.04361\n",
      "Epoch: 8/40 Iteration: 1880 | Train loss: 0.10609\n",
      "Epoch: 8/40 Iteration: 1900 | Train loss: 0.06021\n",
      "Epoch: 8/40 Iteration: 1920 | Train loss: 0.14838\n",
      "Epoch: 8/40 Iteration: 1940 | Train loss: 0.05706\n",
      "Epoch: 8/40 Iteration: 1960 | Train loss: 0.01897\n",
      "Epoch: 8/40 Iteration: 1980 | Train loss: 0.01627\n",
      "Epoch: 8/40 Iteration: 2000 | Train loss: 0.09080\n",
      "Epoch: 9/40 Iteration: 2020 | Train loss: 0.05685\n",
      "Epoch: 9/40 Iteration: 2040 | Train loss: 0.03073\n",
      "Epoch: 9/40 Iteration: 2060 | Train loss: 0.02588\n",
      "Epoch: 9/40 Iteration: 2080 | Train loss: 0.05401\n",
      "Epoch: 9/40 Iteration: 2100 | Train loss: 0.05290\n",
      "Epoch: 9/40 Iteration: 2120 | Train loss: 0.03885\n",
      "Epoch: 9/40 Iteration: 2140 | Train loss: 0.06573\n",
      "Epoch: 9/40 Iteration: 2160 | Train loss: 0.09198\n",
      "Epoch: 9/40 Iteration: 2180 | Train loss: 0.06794\n",
      "Epoch: 9/40 Iteration: 2200 | Train loss: 0.07319\n",
      "Epoch: 9/40 Iteration: 2220 | Train loss: 0.10411\n",
      "Epoch: 9/40 Iteration: 2240 | Train loss: 0.09654\n",
      "Epoch: 10/40 Iteration: 2260 | Train loss: 0.08460\n",
      "Epoch: 10/40 Iteration: 2280 | Train loss: 0.10315\n",
      "Epoch: 10/40 Iteration: 2300 | Train loss: 0.02175\n",
      "Epoch: 10/40 Iteration: 2320 | Train loss: 0.14536\n",
      "Epoch: 10/40 Iteration: 2340 | Train loss: 0.06117\n",
      "Epoch: 10/40 Iteration: 2360 | Train loss: 0.06845\n",
      "Epoch: 10/40 Iteration: 2380 | Train loss: 0.05326\n",
      "Epoch: 10/40 Iteration: 2400 | Train loss: 0.01382\n",
      "Epoch: 10/40 Iteration: 2420 | Train loss: 0.03561\n",
      "Epoch: 10/40 Iteration: 2440 | Train loss: 0.08197\n",
      "Epoch: 10/40 Iteration: 2460 | Train loss: 0.02843\n",
      "Epoch: 10/40 Iteration: 2480 | Train loss: 0.07470\n",
      "Epoch: 10/40 Iteration: 2500 | Train loss: 0.03440\n",
      "Epoch: 11/40 Iteration: 2520 | Train loss: 0.03707\n",
      "Epoch: 11/40 Iteration: 2540 | Train loss: 0.01530\n",
      "Epoch: 11/40 Iteration: 2560 | Train loss: 0.01050\n",
      "Epoch: 11/40 Iteration: 2580 | Train loss: 0.01309\n",
      "Epoch: 11/40 Iteration: 2600 | Train loss: 0.01049\n",
      "Epoch: 11/40 Iteration: 2620 | Train loss: 0.01571\n",
      "Epoch: 11/40 Iteration: 2640 | Train loss: 0.00612\n",
      "Epoch: 11/40 Iteration: 2660 | Train loss: 0.00334\n",
      "Epoch: 11/40 Iteration: 2680 | Train loss: 0.03634\n",
      "Epoch: 11/40 Iteration: 2700 | Train loss: 0.00729\n",
      "Epoch: 11/40 Iteration: 2720 | Train loss: 0.14086\n",
      "Epoch: 11/40 Iteration: 2740 | Train loss: 0.00472\n",
      "Epoch: 12/40 Iteration: 2760 | Train loss: 0.08797\n",
      "Epoch: 12/40 Iteration: 2780 | Train loss: 0.00391\n",
      "Epoch: 12/40 Iteration: 2800 | Train loss: 0.00644\n",
      "Epoch: 12/40 Iteration: 2820 | Train loss: 0.02419\n",
      "Epoch: 12/40 Iteration: 2840 | Train loss: 0.01612\n",
      "Epoch: 12/40 Iteration: 2860 | Train loss: 0.00223\n",
      "Epoch: 12/40 Iteration: 2880 | Train loss: 0.00232\n",
      "Epoch: 12/40 Iteration: 2900 | Train loss: 0.03676\n",
      "Epoch: 12/40 Iteration: 2920 | Train loss: 0.01775\n",
      "Epoch: 12/40 Iteration: 2940 | Train loss: 0.02423\n",
      "Epoch: 12/40 Iteration: 2960 | Train loss: 0.03676\n",
      "Epoch: 12/40 Iteration: 2980 | Train loss: 0.00468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/40 Iteration: 3000 | Train loss: 0.11784\n",
      "Epoch: 13/40 Iteration: 3020 | Train loss: 0.01523\n",
      "Epoch: 13/40 Iteration: 3040 | Train loss: 0.12908\n",
      "Epoch: 13/40 Iteration: 3060 | Train loss: 0.00325\n",
      "Epoch: 13/40 Iteration: 3080 | Train loss: 0.00189\n",
      "Epoch: 13/40 Iteration: 3100 | Train loss: 0.00367\n",
      "Epoch: 13/40 Iteration: 3120 | Train loss: 0.00220\n",
      "Epoch: 13/40 Iteration: 3140 | Train loss: 0.00392\n",
      "Epoch: 13/40 Iteration: 3160 | Train loss: 0.01693\n",
      "Epoch: 13/40 Iteration: 3180 | Train loss: 0.00748\n",
      "Epoch: 13/40 Iteration: 3200 | Train loss: 0.00673\n",
      "Epoch: 13/40 Iteration: 3220 | Train loss: 0.02451\n",
      "Epoch: 13/40 Iteration: 3240 | Train loss: 0.03398\n",
      "Epoch: 14/40 Iteration: 3260 | Train loss: 0.06345\n",
      "Epoch: 14/40 Iteration: 3280 | Train loss: 0.00763\n",
      "Epoch: 14/40 Iteration: 3300 | Train loss: 0.00770\n",
      "Epoch: 14/40 Iteration: 3320 | Train loss: 0.00326\n",
      "Epoch: 14/40 Iteration: 3340 | Train loss: 0.01327\n",
      "Epoch: 14/40 Iteration: 3360 | Train loss: 0.00532\n",
      "Epoch: 14/40 Iteration: 3380 | Train loss: 0.02460\n",
      "Epoch: 14/40 Iteration: 3400 | Train loss: 0.00076\n",
      "Epoch: 14/40 Iteration: 3420 | Train loss: 0.00214\n",
      "Epoch: 14/40 Iteration: 3440 | Train loss: 0.00162\n",
      "Epoch: 14/40 Iteration: 3460 | Train loss: 0.00767\n",
      "Epoch: 14/40 Iteration: 3480 | Train loss: 0.00277\n",
      "Epoch: 14/40 Iteration: 3500 | Train loss: 0.01206\n",
      "Epoch: 15/40 Iteration: 3520 | Train loss: 0.05375\n",
      "Epoch: 15/40 Iteration: 3540 | Train loss: 0.01021\n",
      "Epoch: 15/40 Iteration: 3560 | Train loss: 0.00646\n",
      "Epoch: 15/40 Iteration: 3580 | Train loss: 0.01933\n",
      "Epoch: 15/40 Iteration: 3600 | Train loss: 0.00808\n",
      "Epoch: 15/40 Iteration: 3620 | Train loss: 0.00164\n",
      "Epoch: 15/40 Iteration: 3640 | Train loss: 0.00324\n",
      "Epoch: 15/40 Iteration: 3660 | Train loss: 0.00247\n",
      "Epoch: 15/40 Iteration: 3680 | Train loss: 0.00410\n",
      "Epoch: 15/40 Iteration: 3700 | Train loss: 0.00076\n",
      "Epoch: 15/40 Iteration: 3720 | Train loss: 0.00471\n",
      "Epoch: 15/40 Iteration: 3740 | Train loss: 0.01838\n",
      "Epoch: 16/40 Iteration: 3760 | Train loss: 0.02388\n",
      "Epoch: 16/40 Iteration: 3780 | Train loss: 0.01449\n",
      "Epoch: 16/40 Iteration: 3800 | Train loss: 0.00043\n",
      "Epoch: 16/40 Iteration: 3820 | Train loss: 0.00272\n",
      "Epoch: 16/40 Iteration: 3840 | Train loss: 0.00319\n",
      "Epoch: 16/40 Iteration: 3860 | Train loss: 0.00702\n",
      "Epoch: 16/40 Iteration: 3880 | Train loss: 0.00089\n",
      "Epoch: 16/40 Iteration: 3900 | Train loss: 0.01943\n",
      "Epoch: 16/40 Iteration: 3920 | Train loss: 0.00038\n",
      "Epoch: 16/40 Iteration: 3940 | Train loss: 0.00105\n",
      "Epoch: 16/40 Iteration: 3960 | Train loss: 0.00075\n",
      "Epoch: 16/40 Iteration: 3980 | Train loss: 0.00800\n",
      "Epoch: 16/40 Iteration: 4000 | Train loss: 0.01963\n",
      "Epoch: 17/40 Iteration: 4020 | Train loss: 0.00518\n",
      "Epoch: 17/40 Iteration: 4040 | Train loss: 0.00442\n",
      "Epoch: 17/40 Iteration: 4060 | Train loss: 0.00085\n",
      "Epoch: 17/40 Iteration: 4080 | Train loss: 0.00024\n",
      "Epoch: 17/40 Iteration: 4100 | Train loss: 0.00209\n",
      "Epoch: 17/40 Iteration: 4120 | Train loss: 0.00141\n",
      "Epoch: 17/40 Iteration: 4140 | Train loss: 0.00088\n",
      "Epoch: 17/40 Iteration: 4160 | Train loss: 0.00104\n",
      "Epoch: 17/40 Iteration: 4180 | Train loss: 0.00092\n",
      "Epoch: 17/40 Iteration: 4200 | Train loss: 0.00053\n",
      "Epoch: 17/40 Iteration: 4220 | Train loss: 0.00142\n",
      "Epoch: 17/40 Iteration: 4240 | Train loss: 0.00086\n",
      "Epoch: 18/40 Iteration: 4260 | Train loss: 0.00214\n",
      "Epoch: 18/40 Iteration: 4280 | Train loss: 0.00116\n",
      "Epoch: 18/40 Iteration: 4300 | Train loss: 0.00038\n",
      "Epoch: 18/40 Iteration: 4320 | Train loss: 0.00031\n",
      "Epoch: 18/40 Iteration: 4340 | Train loss: 0.00467\n",
      "Epoch: 18/40 Iteration: 4360 | Train loss: 0.00046\n",
      "Epoch: 18/40 Iteration: 4380 | Train loss: 0.00047\n",
      "Epoch: 18/40 Iteration: 4400 | Train loss: 0.00014\n",
      "Epoch: 18/40 Iteration: 4420 | Train loss: 0.00008\n",
      "Epoch: 18/40 Iteration: 4440 | Train loss: 0.00049\n",
      "Epoch: 18/40 Iteration: 4460 | Train loss: 0.00019\n",
      "Epoch: 18/40 Iteration: 4480 | Train loss: 0.00011\n",
      "Epoch: 18/40 Iteration: 4500 | Train loss: 0.00014\n",
      "Epoch: 19/40 Iteration: 4520 | Train loss: 0.00124\n",
      "Epoch: 19/40 Iteration: 4540 | Train loss: 0.00063\n",
      "Epoch: 19/40 Iteration: 4560 | Train loss: 0.00008\n",
      "Epoch: 19/40 Iteration: 4580 | Train loss: 0.00020\n",
      "Epoch: 19/40 Iteration: 4600 | Train loss: 0.00042\n",
      "Epoch: 19/40 Iteration: 4620 | Train loss: 0.00084\n",
      "Epoch: 19/40 Iteration: 4640 | Train loss: 0.00052\n",
      "Epoch: 19/40 Iteration: 4660 | Train loss: 0.00015\n",
      "Epoch: 19/40 Iteration: 4680 | Train loss: 0.00048\n",
      "Epoch: 19/40 Iteration: 4700 | Train loss: 0.00038\n",
      "Epoch: 19/40 Iteration: 4720 | Train loss: 0.00006\n",
      "Epoch: 19/40 Iteration: 4740 | Train loss: 0.00007\n",
      "Epoch: 20/40 Iteration: 4760 | Train loss: 0.00044\n",
      "Epoch: 20/40 Iteration: 4780 | Train loss: 0.00106\n",
      "Epoch: 20/40 Iteration: 4800 | Train loss: 0.00132\n",
      "Epoch: 20/40 Iteration: 4820 | Train loss: 0.00087\n",
      "Epoch: 20/40 Iteration: 4840 | Train loss: 0.00032\n",
      "Epoch: 20/40 Iteration: 4860 | Train loss: 0.02881\n",
      "Epoch: 20/40 Iteration: 4880 | Train loss: 0.00120\n",
      "Epoch: 20/40 Iteration: 4900 | Train loss: 0.00038\n",
      "Epoch: 20/40 Iteration: 4920 | Train loss: 0.02456\n",
      "Epoch: 20/40 Iteration: 4940 | Train loss: 0.00515\n",
      "Epoch: 20/40 Iteration: 4960 | Train loss: 0.00257\n",
      "Epoch: 20/40 Iteration: 4980 | Train loss: 0.00164\n",
      "Epoch: 20/40 Iteration: 5000 | Train loss: 0.00375\n",
      "Epoch: 21/40 Iteration: 5020 | Train loss: 0.00360\n",
      "Epoch: 21/40 Iteration: 5040 | Train loss: 0.02411\n",
      "Epoch: 21/40 Iteration: 5060 | Train loss: 0.00125\n",
      "Epoch: 21/40 Iteration: 5080 | Train loss: 0.02652\n",
      "Epoch: 21/40 Iteration: 5100 | Train loss: 0.02537\n",
      "Epoch: 21/40 Iteration: 5120 | Train loss: 0.01035\n",
      "Epoch: 21/40 Iteration: 5140 | Train loss: 0.00725\n",
      "Epoch: 21/40 Iteration: 5160 | Train loss: 0.00212\n",
      "Epoch: 21/40 Iteration: 5180 | Train loss: 0.00271\n",
      "Epoch: 21/40 Iteration: 5200 | Train loss: 0.01311\n",
      "Epoch: 21/40 Iteration: 5220 | Train loss: 0.00542\n",
      "Epoch: 21/40 Iteration: 5240 | Train loss: 0.01671\n",
      "Epoch: 22/40 Iteration: 5260 | Train loss: 0.00952\n",
      "Epoch: 22/40 Iteration: 5280 | Train loss: 0.01544\n",
      "Epoch: 22/40 Iteration: 5300 | Train loss: 0.00328\n",
      "Epoch: 22/40 Iteration: 5320 | Train loss: 0.01018\n",
      "Epoch: 22/40 Iteration: 5340 | Train loss: 0.00364\n",
      "Epoch: 22/40 Iteration: 5360 | Train loss: 0.00236\n",
      "Epoch: 22/40 Iteration: 5380 | Train loss: 0.04139\n",
      "Epoch: 22/40 Iteration: 5400 | Train loss: 0.02363\n",
      "Epoch: 22/40 Iteration: 5420 | Train loss: 0.00597\n",
      "Epoch: 22/40 Iteration: 5440 | Train loss: 0.00234\n",
      "Epoch: 22/40 Iteration: 5460 | Train loss: 0.00736\n",
      "Epoch: 22/40 Iteration: 5480 | Train loss: 0.01278\n",
      "Epoch: 22/40 Iteration: 5500 | Train loss: 0.01441\n",
      "Epoch: 23/40 Iteration: 5520 | Train loss: 0.00141\n",
      "Epoch: 23/40 Iteration: 5540 | Train loss: 0.00536\n",
      "Epoch: 23/40 Iteration: 5560 | Train loss: 0.00041\n",
      "Epoch: 23/40 Iteration: 5580 | Train loss: 0.00432\n",
      "Epoch: 23/40 Iteration: 5600 | Train loss: 0.00055\n",
      "Epoch: 23/40 Iteration: 5620 | Train loss: 0.00939\n",
      "Epoch: 23/40 Iteration: 5640 | Train loss: 0.01242\n",
      "Epoch: 23/40 Iteration: 5660 | Train loss: 0.00529\n",
      "Epoch: 23/40 Iteration: 5680 | Train loss: 0.00172\n",
      "Epoch: 23/40 Iteration: 5700 | Train loss: 0.00436\n",
      "Epoch: 23/40 Iteration: 5720 | Train loss: 0.00022\n",
      "Epoch: 23/40 Iteration: 5740 | Train loss: 0.00311\n",
      "Epoch: 24/40 Iteration: 5760 | Train loss: 0.00088\n",
      "Epoch: 24/40 Iteration: 5780 | Train loss: 0.00028\n",
      "Epoch: 24/40 Iteration: 5800 | Train loss: 0.00051\n",
      "Epoch: 24/40 Iteration: 5820 | Train loss: 0.00718\n",
      "Epoch: 24/40 Iteration: 5840 | Train loss: 0.01315\n",
      "Epoch: 24/40 Iteration: 5860 | Train loss: 0.00306\n",
      "Epoch: 24/40 Iteration: 5880 | Train loss: 0.00511\n",
      "Epoch: 24/40 Iteration: 5900 | Train loss: 0.00034\n",
      "Epoch: 24/40 Iteration: 5920 | Train loss: 0.00203\n",
      "Epoch: 24/40 Iteration: 5940 | Train loss: 0.00214\n",
      "Epoch: 24/40 Iteration: 5960 | Train loss: 0.00392\n",
      "Epoch: 24/40 Iteration: 5980 | Train loss: 0.00186\n",
      "Epoch: 24/40 Iteration: 6000 | Train loss: 0.00110\n",
      "Epoch: 25/40 Iteration: 6020 | Train loss: 0.00143\n",
      "Epoch: 25/40 Iteration: 6040 | Train loss: 0.00294\n",
      "Epoch: 25/40 Iteration: 6060 | Train loss: 0.00883\n",
      "Epoch: 25/40 Iteration: 6080 | Train loss: 0.00058\n",
      "Epoch: 25/40 Iteration: 6100 | Train loss: 0.00107\n",
      "Epoch: 25/40 Iteration: 6120 | Train loss: 0.00069\n",
      "Epoch: 25/40 Iteration: 6140 | Train loss: 0.00034\n",
      "Epoch: 25/40 Iteration: 6160 | Train loss: 0.00060\n",
      "Epoch: 25/40 Iteration: 6180 | Train loss: 0.00197\n",
      "Epoch: 25/40 Iteration: 6200 | Train loss: 0.00063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/40 Iteration: 6220 | Train loss: 0.00111\n",
      "Epoch: 25/40 Iteration: 6240 | Train loss: 0.00093\n",
      "Epoch: 26/40 Iteration: 6260 | Train loss: 0.00025\n",
      "Epoch: 26/40 Iteration: 6280 | Train loss: 0.00018\n",
      "Epoch: 26/40 Iteration: 6300 | Train loss: 0.00050\n",
      "Epoch: 26/40 Iteration: 6320 | Train loss: 0.00008\n",
      "Epoch: 26/40 Iteration: 6340 | Train loss: 0.00046\n",
      "Epoch: 26/40 Iteration: 6360 | Train loss: 0.01553\n",
      "Epoch: 26/40 Iteration: 6380 | Train loss: 0.02575\n",
      "Epoch: 26/40 Iteration: 6400 | Train loss: 0.00525\n",
      "Epoch: 26/40 Iteration: 6420 | Train loss: 0.00086\n",
      "Epoch: 26/40 Iteration: 6440 | Train loss: 0.00104\n",
      "Epoch: 26/40 Iteration: 6460 | Train loss: 0.00087\n",
      "Epoch: 26/40 Iteration: 6480 | Train loss: 0.00071\n",
      "Epoch: 26/40 Iteration: 6500 | Train loss: 0.00068\n",
      "Epoch: 27/40 Iteration: 6520 | Train loss: 0.00082\n",
      "Epoch: 27/40 Iteration: 6540 | Train loss: 0.00551\n",
      "Epoch: 27/40 Iteration: 6560 | Train loss: 0.00029\n",
      "Epoch: 27/40 Iteration: 6580 | Train loss: 0.00011\n",
      "Epoch: 27/40 Iteration: 6600 | Train loss: 0.00029\n",
      "Epoch: 27/40 Iteration: 6620 | Train loss: 0.00042\n",
      "Epoch: 27/40 Iteration: 6640 | Train loss: 0.00015\n",
      "Epoch: 27/40 Iteration: 6660 | Train loss: 0.00031\n",
      "Epoch: 27/40 Iteration: 6680 | Train loss: 0.00041\n",
      "Epoch: 27/40 Iteration: 6700 | Train loss: 0.00055\n",
      "Epoch: 27/40 Iteration: 6720 | Train loss: 0.00237\n",
      "Epoch: 27/40 Iteration: 6740 | Train loss: 0.00023\n",
      "Epoch: 28/40 Iteration: 6760 | Train loss: 0.00004\n",
      "Epoch: 28/40 Iteration: 6780 | Train loss: 0.00009\n",
      "Epoch: 28/40 Iteration: 6800 | Train loss: 0.00017\n",
      "Epoch: 28/40 Iteration: 6820 | Train loss: 0.00015\n",
      "Epoch: 28/40 Iteration: 6840 | Train loss: 0.00012\n",
      "Epoch: 28/40 Iteration: 6860 | Train loss: 0.00016\n",
      "Epoch: 28/40 Iteration: 6880 | Train loss: 0.00024\n",
      "Epoch: 28/40 Iteration: 6900 | Train loss: 0.00025\n",
      "Epoch: 28/40 Iteration: 6920 | Train loss: 0.00009\n",
      "Epoch: 28/40 Iteration: 6940 | Train loss: 0.00012\n",
      "Epoch: 28/40 Iteration: 6960 | Train loss: 0.00010\n",
      "Epoch: 28/40 Iteration: 6980 | Train loss: 0.00007\n",
      "Epoch: 28/40 Iteration: 7000 | Train loss: 0.00012\n",
      "Epoch: 29/40 Iteration: 7020 | Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7040 | Train loss: 0.00010\n",
      "Epoch: 29/40 Iteration: 7060 | Train loss: 0.00002\n",
      "Epoch: 29/40 Iteration: 7080 | Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7100 | Train loss: 0.00038\n",
      "Epoch: 29/40 Iteration: 7120 | Train loss: 0.00006\n",
      "Epoch: 29/40 Iteration: 7140 | Train loss: 0.00012\n",
      "Epoch: 29/40 Iteration: 7160 | Train loss: 0.00014\n",
      "Epoch: 29/40 Iteration: 7180 | Train loss: 0.00028\n",
      "Epoch: 29/40 Iteration: 7200 | Train loss: 0.00017\n",
      "Epoch: 29/40 Iteration: 7220 | Train loss: 0.00012\n",
      "Epoch: 29/40 Iteration: 7240 | Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7260 | Train loss: 0.00004\n",
      "Epoch: 30/40 Iteration: 7280 | Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7300 | Train loss: 0.00004\n",
      "Epoch: 30/40 Iteration: 7320 | Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7340 | Train loss: 0.00007\n",
      "Epoch: 30/40 Iteration: 7360 | Train loss: 0.00006\n",
      "Epoch: 30/40 Iteration: 7380 | Train loss: 0.00003\n",
      "Epoch: 30/40 Iteration: 7400 | Train loss: 0.00011\n",
      "Epoch: 30/40 Iteration: 7420 | Train loss: 0.00010\n",
      "Epoch: 30/40 Iteration: 7440 | Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7460 | Train loss: 0.00004\n",
      "Epoch: 30/40 Iteration: 7480 | Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7500 | Train loss: 0.00007\n",
      "Epoch: 31/40 Iteration: 7520 | Train loss: 0.00002\n",
      "Epoch: 31/40 Iteration: 7540 | Train loss: 0.00013\n",
      "Epoch: 31/40 Iteration: 7560 | Train loss: 0.00002\n",
      "Epoch: 31/40 Iteration: 7580 | Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7600 | Train loss: 0.00015\n",
      "Epoch: 31/40 Iteration: 7620 | Train loss: 0.00008\n",
      "Epoch: 31/40 Iteration: 7640 | Train loss: 0.00035\n",
      "Epoch: 31/40 Iteration: 7660 | Train loss: 0.00003\n",
      "Epoch: 31/40 Iteration: 7680 | Train loss: 0.00005\n",
      "Epoch: 31/40 Iteration: 7700 | Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7720 | Train loss: 0.00005\n",
      "Epoch: 31/40 Iteration: 7740 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7760 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7780 | Train loss: 0.00003\n",
      "Epoch: 32/40 Iteration: 7800 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7820 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7840 | Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7860 | Train loss: 0.00003\n",
      "Epoch: 32/40 Iteration: 7880 | Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7900 | Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7920 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7940 | Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7960 | Train loss: 0.00003\n",
      "Epoch: 32/40 Iteration: 7980 | Train loss: 0.00017\n",
      "Epoch: 32/40 Iteration: 8000 | Train loss: 0.00002\n",
      "Epoch: 33/40 Iteration: 8020 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8040 | Train loss: 0.00016\n",
      "Epoch: 33/40 Iteration: 8060 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8080 | Train loss: 0.00020\n",
      "Epoch: 33/40 Iteration: 8100 | Train loss: 0.00011\n",
      "Epoch: 33/40 Iteration: 8120 | Train loss: 0.00005\n",
      "Epoch: 33/40 Iteration: 8140 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8160 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8180 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8200 | Train loss: 0.00003\n",
      "Epoch: 33/40 Iteration: 8220 | Train loss: 0.00003\n",
      "Epoch: 33/40 Iteration: 8240 | Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8260 | Train loss: 0.00003\n",
      "Epoch: 34/40 Iteration: 8280 | Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8300 | Train loss: 0.00003\n",
      "Epoch: 34/40 Iteration: 8320 | Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8340 | Train loss: 0.00003\n",
      "Epoch: 34/40 Iteration: 8360 | Train loss: 0.00005\n",
      "Epoch: 34/40 Iteration: 8380 | Train loss: 0.00004\n",
      "Epoch: 34/40 Iteration: 8400 | Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8420 | Train loss: 0.00003\n",
      "Epoch: 34/40 Iteration: 8440 | Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8460 | Train loss: 0.00003\n",
      "Epoch: 34/40 Iteration: 8480 | Train loss: 0.00023\n",
      "Epoch: 34/40 Iteration: 8500 | Train loss: 0.00060\n",
      "Epoch: 35/40 Iteration: 8520 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8540 | Train loss: 0.00005\n",
      "Epoch: 35/40 Iteration: 8560 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8580 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8600 | Train loss: 0.00015\n",
      "Epoch: 35/40 Iteration: 8620 | Train loss: 0.00003\n",
      "Epoch: 35/40 Iteration: 8640 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8660 | Train loss: 0.00002\n",
      "Epoch: 35/40 Iteration: 8680 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8700 | Train loss: 0.00011\n",
      "Epoch: 35/40 Iteration: 8720 | Train loss: 0.00002\n",
      "Epoch: 35/40 Iteration: 8740 | Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8760 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8780 | Train loss: 0.00002\n",
      "Epoch: 36/40 Iteration: 8800 | Train loss: 0.00002\n",
      "Epoch: 36/40 Iteration: 8820 | Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8840 | Train loss: 0.00005\n",
      "Epoch: 36/40 Iteration: 8860 | Train loss: 0.00003\n",
      "Epoch: 36/40 Iteration: 8880 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8900 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8920 | Train loss: 0.00002\n",
      "Epoch: 36/40 Iteration: 8940 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8960 | Train loss: 0.00002\n",
      "Epoch: 36/40 Iteration: 8980 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 9000 | Train loss: 0.00002\n",
      "Epoch: 37/40 Iteration: 9020 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9040 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9060 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9080 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9100 | Train loss: 0.00004\n",
      "Epoch: 37/40 Iteration: 9120 | Train loss: 0.00005\n",
      "Epoch: 37/40 Iteration: 9140 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9160 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9180 | Train loss: 0.00007\n",
      "Epoch: 37/40 Iteration: 9200 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9220 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9240 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9260 | Train loss: 0.00000\n",
      "Epoch: 38/40 Iteration: 9280 | Train loss: 0.00002\n",
      "Epoch: 38/40 Iteration: 9300 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9320 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9340 | Train loss: 0.00002\n",
      "Epoch: 38/40 Iteration: 9360 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9380 | Train loss: 0.00000\n",
      "Epoch: 38/40 Iteration: 9400 | Train loss: 0.00002\n",
      "Epoch: 38/40 Iteration: 9420 | Train loss: 0.00001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/40 Iteration: 9440 | Train loss: 0.00002\n",
      "Epoch: 38/40 Iteration: 9460 | Train loss: 0.00001\n",
      "Epoch: 38/40 Iteration: 9480 | Train loss: 0.00003\n",
      "Epoch: 38/40 Iteration: 9500 | Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9520 | Train loss: 0.00000\n",
      "Epoch: 39/40 Iteration: 9540 | Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9560 | Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9580 | Train loss: 0.00000\n",
      "Epoch: 39/40 Iteration: 9600 | Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9620 | Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9640 | Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9660 | Train loss: 0.00001\n",
      "Epoch: 39/40 Iteration: 9680 | Train loss: 0.00000\n",
      "Epoch: 39/40 Iteration: 9700 | Train loss: 0.00003\n",
      "Epoch: 39/40 Iteration: 9720 | Train loss: 0.00000\n",
      "Epoch: 39/40 Iteration: 9740 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9760 | Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 9780 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9800 | Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 9820 | Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 9840 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9860 | Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 9880 | Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 9900 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9920 | Train loss: 0.00000\n",
      "Epoch: 40/40 Iteration: 9940 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9960 | Train loss: 0.00001\n",
      "Epoch: 40/40 Iteration: 9980 | Train loss: 0.00003\n",
      "Epoch: 40/40 Iteration: 10000 | Train loss: 0.00000\n"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words, \n",
    "                   seq_len=sequence_length,\n",
    "                   embed_size=256, \n",
    "                   lstm_size=128, \n",
    "                   num_layers=1, \n",
    "                   batch_size=100, \n",
    "                   learning_rate=0.001)\n",
    "\n",
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/sentiment-39.ckpt\n",
      "Test Acc.: 0.856\n"
     ]
    }
   ],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: %.3f' % (\n",
    "      np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing tha data - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "## Reading and processing text\n",
    "with open('/home/tako/hyungjinko/fintech/pg2265.txt', 'r') as f: \n",
    "    text=f.read()\n",
    "\n",
    "text = text[15858:]\n",
    "chars = set(text)\n",
    "char2int = {ch:i for i,ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text], \n",
    "                     dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing tha data - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    tot_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / tot_batch_length)\n",
    "    if num_batches*tot_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "    ## Truncate the sequence at the end to get rid of \n",
    "    ## remaining charcaters that do not make a full batch\n",
    "    x = sequence[0 : num_batches*tot_batch_length]\n",
    "    y = sequence[1 : num_batches*tot_batch_length + 1]\n",
    "    ## Split x & y into a list batches of sequences: \n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    ## Stack the batches together\n",
    "    ## batch_size x tot_batch_length\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "## Testing:\n",
    "train_x, train_y = reshape_data(text_ints, 64, 10)\n",
    "print(train_x.shape)\n",
    "print(train_x[0, :10])\n",
    "print(train_y[0, :10])\n",
    "print(''.join(int2char[i] for i in train_x[0, :50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing tha data - 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape    \n",
    "    num_batches = int(tot_batch_length/num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield (data_x[:, b*num_steps: (b+1)*num_steps], \n",
    "               data_y[:, b*num_steps: (b+1)*num_steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, num_classes, batch_size=64, \n",
    "                 num_steps=100, lstm_size=128, \n",
    "                 num_layers=1, learning_rate=0.001, \n",
    "                 keep_prob=0.5, grad_clip=5, \n",
    "                 sampling=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "\n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            \n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        tf_x = tf.placeholder(tf.int32, \n",
    "                              shape=[batch_size, num_steps], \n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, \n",
    "                              shape=[batch_size, num_steps], \n",
    "                              name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32, \n",
    "                              name='tf_keepprob')\n",
    "\n",
    "        # One-hot encoding:\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "\n",
    "        ### Build the multi-layer RNN cells\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.lstm_size), \n",
    "                output_keep_prob=tf_keepprob) \n",
    "            for _ in range(self.num_layers)])\n",
    "        \n",
    "        ## Define the initial state\n",
    "        self.initial_state = cells.zero_state(\n",
    "                    batch_size, tf.float32)\n",
    "\n",
    "        ## Run each sequence step through the RNN \n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                    cells, x_onehot, \n",
    "                    initial_state=self.initial_state)\n",
    "        \n",
    "        print('  << lstm_outputs  >>', lstm_outputs)\n",
    "\n",
    "        seq_output_reshaped = tf.reshape(\n",
    "                    lstm_outputs, \n",
    "                    shape=[-1, self.lstm_size],\n",
    "                    name='seq_output_reshaped')\n",
    "\n",
    "        logits = tf.layers.dense(\n",
    "                    inputs=seq_output_reshaped, \n",
    "                    units=self.num_classes,\n",
    "                    activation=None,\n",
    "                    name='logits')\n",
    "\n",
    "        proba = tf.nn.softmax(\n",
    "                    logits, \n",
    "                    name='probabilities')\n",
    "        print(proba)\n",
    "\n",
    "        y_reshaped = tf.reshape(\n",
    "                    y_onehot, \n",
    "                    shape=[-1, self.num_classes],\n",
    "                    name='y_reshaped')\n",
    "        cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=logits, \n",
    "                        labels=y_reshaped),\n",
    "                    name='cost')\n",
    "\n",
    "        # Gradient clipping to avoid \"exploding gradients\"\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "                    tf.gradients(cost, tvars), \n",
    "                    self.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "                    zip(grads, tvars),\n",
    "                    name='train_op')\n",
    "        \n",
    "    def train(self, train_x, train_y, \n",
    "              num_epochs, ckpt_dir='./model/'):\n",
    "        ## Create the checkpoint directory\n",
    "        ## if does not exists\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "            \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "\n",
    "            n_batches = int(train_x.shape[1]/self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                # Train network\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "                ## Minibatch generator:\n",
    "                bgen = create_batch_generator(\n",
    "                        train_x, train_y, self.num_steps)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch*n_batches + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': self.keep_prob,\n",
    "                            self.initial_state : new_state}\n",
    "                    batch_cost, _, new_state = sess.run(\n",
    "                            ['cost:0', 'train_op', \n",
    "                                self.final_state],\n",
    "                            feed_dict=feed)\n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch %d/%d Iteration %d'\n",
    "                              '| Training loss: %.4f' % (\n",
    "                              epoch + 1, num_epochs, \n",
    "                              iteration, batch_cost))\n",
    "\n",
    "                ## Save the trained model    \n",
    "                self.saver.save(\n",
    "                        sess, os.path.join(\n",
    "                            ckpt_dir, 'language_modeling.ckpt'))\n",
    "                              \n",
    "                              \n",
    "                \n",
    "    def sample(self, output_length, \n",
    "               ckpt_dir, starter_seq=\"The \"):\n",
    "        observed_seq = [ch for ch in starter_seq]        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, \n",
    "                tf.train.latest_checkpoint(ckpt_dir))\n",
    "            ## 1: run the model using the starter sequence\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0,0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state], \n",
    "                        feed_dict=feed)\n",
    "\n",
    "            ch_id = get_top_char(proba, len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "            \n",
    "            ## 2: run the model using the updated observed_seq\n",
    "            for i in range(output_length):\n",
    "                x[0,0] = ch_id\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state], \n",
    "                        feed_dict=feed)\n",
    "\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "        return ''.join(observed_seq)\n",
    "    \n",
    "\n",
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training the CharRNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100 \n",
    "train_x, train_y = reshape_data(text_ints, \n",
    "                                batch_size, \n",
    "                                num_steps)\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y, \n",
    "          num_epochs=100,\n",
    "          ckpt_dir='./model-100/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CharRNN model in the sampling mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del rnn\n",
    "\n",
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "\n",
    "print(rnn.sample(ckpt_dir='./model-100/', \n",
    "                 output_length=500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
